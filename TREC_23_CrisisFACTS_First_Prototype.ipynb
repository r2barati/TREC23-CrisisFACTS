{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqY2ojYHt0+PsCupDBVRy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r2barati/TREC23-CrisisFACTS/blob/main/TREC_23_CrisisFACTS_First_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDmrJaQUkQeo"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/allenai/ir_datasets.git@crisisfacts # install ir_datasets (crisisfacts branch)\n",
        "\n",
        "credentials = {\n",
        "    \"institution\": \"<Toronto Metropolitan University>\", # University, Company or Public Agency Name\n",
        "    \"contactname\": \"<Reza Barati, Aary Kartha>\", # Your Name\n",
        "    \"email\": \"<rezabarati@gmail.com, aaryaman.kartha@torontomu.ca>\", # A contact email address\n",
        "    \"institutiontype\": \"<Research>\" # Either 'Research', 'Industry', or 'Public Sector'\n",
        "}\n",
        "\n",
        "# Write this to a file so it can be read when needed\n",
        "import json\n",
        "import os\n",
        "\n",
        "home_dir = os.path.expanduser('~')\n",
        "\n",
        "!mkdir -p ~/.ir_datasets/auth/\n",
        "with open(home_dir + '/.ir_datasets/auth/crisisfacts.json', 'w') as f:\n",
        "    json.dump(credentials, f)\n",
        "\n",
        "# Event numbers as a list\n",
        "eventNoList = [\n",
        "    \"001\", # Lilac Wildfire 2017\n",
        "    \"002\", # Cranston Wildfire 2018\n",
        "    \"003\", # Holy Wildfire 2018\n",
        "    \"004\", # Hurricane Florence 2018\n",
        "    \"005\", # 2018 Maryland Flood\n",
        "    \"006\", # Saddleridge Wildfire 2019\n",
        "    \"007\", # Hurricane Laura 2020\n",
        "    \"008\", # Hurricane Sally 2020\n",
        "    \"009\", # Beirut Explosion, 2020\n",
        "    \"010\", # Houston Explosion, 2020\n",
        "    \"011\", # Rutherford TN Floods, 2020\n",
        "    \"012\", # TN Derecho, 2020\n",
        "    \"013\", # Edenville Dam Fail, 2020\n",
        "    \"014\", # Hurricane Dorian, 2019\n",
        "    \"015\", # Kincade Wildfire, 2019\n",
        "    \"016\", # Easter Tornado Outbreak, 2020\n",
        "    \"017\", # Tornado Outbreak, 2020 Apr\n",
        "    \"018\", # Tornado Outbreak, 2020 March\n",
        "]\n",
        "\n",
        "import requests\n",
        "\n",
        "# Gets the list of days for a specified event number, e.g. '001'\n",
        "def getDaysForEventNo(eventNo):\n",
        "\n",
        "    # We will download a file containing the day list for an event\n",
        "    url = \"http://trecis.org/CrisisFACTs/CrisisFACTS-\"+eventNo+\".requests.json\"\n",
        "\n",
        "    # Download the list and parse as JSON\n",
        "    dayList = requests.get(url).json()\n",
        "\n",
        "    # Print each day\n",
        "    # Note each day object contains the following fields\n",
        "    #   {\n",
        "    #      \"eventID\" : \"CrisisFACTS-001\",\n",
        "    #      \"requestID\" : \"CrisisFACTS-001-r3\",\n",
        "    #      \"dateString\" : \"2017-12-07\",\n",
        "    #      \"startUnixTimestamp\" : 1512604800,\n",
        "    #      \"endUnixTimestamp\" : 1512691199\n",
        "    #   }\n",
        "\n",
        "    return dayList\n",
        "\n",
        "for day in getDaysForEventNo(eventNoList[0]):\n",
        "    print(day[\"dateString\"])\n",
        "\n",
        "eventsMeta = {}\n",
        "\n",
        "for eventNo in eventNoList: # for each event\n",
        "    dailyInfo = getDaysForEventNo(eventNo) # get the list of days\n",
        "    eventsMeta[eventNo]= dailyInfo\n",
        "\n",
        "    print(\"Event \"+eventNo)\n",
        "    for day in dailyInfo: # for each day\n",
        "        print(\"  crisisfacts/\"+eventNo+\"/\"+day[\"dateString\"], \"-->\", day[\"requestID\"]) # construct the request string\n",
        "\n",
        "    print()\n",
        "\n",
        "import ir_datasets\n",
        "\n",
        "# download the first day for event 001 (this is a lazy call, it won't download until we first request a document from the stream)\n",
        "dataset = ir_datasets.load('crisisfacts/001/2017-12-07')\n",
        "\n",
        "for item in dataset.docs_iter()[:10]: # create an iterator over the stream containing the first 10 items\n",
        "    print(item)\n",
        "\n",
        "# download the second day for event 009, first 2023 event\n",
        "dataset = ir_datasets.load('crisisfacts/009/2020-08-04')\n",
        "\n",
        "for item in dataset.docs_iter()[:10]: # create an iterator over the stream containing the first 10 items\n",
        "    print(item)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the stream of items to a Pandas Dataframe\n",
        "itemsAsDataFrame = pd.DataFrame(dataset.docs_iter())\n",
        "\n",
        "# Create a filter expression\n",
        "is_reddit =  itemsAsDataFrame['source_type']==\"Reddit\"\n",
        "\n",
        "# Apply our filter\n",
        "itemsAsDataFrame[is_reddit]\n",
        "\n",
        "# Create a filter expression\n",
        "is_twitter =  itemsAsDataFrame['source_type']==\"Twitter\"\n",
        "\n",
        "# Apply our filter\n",
        "itemsAsDataFrame[is_twitter]\n",
        "\n",
        "# Create a filter expression\n",
        "is_fb =  itemsAsDataFrame['source_type']==\"Facebook\"\n",
        "\n",
        "# Apply our filter\n",
        "itemsAsDataFrame[is_fb]\n",
        "\n",
        "# Create a filter expression\n",
        "is_news =  itemsAsDataFrame['source_type']==\"News\"\n",
        "\n",
        "# Apply our filter\n",
        "itemsAsDataFrame[is_news]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(dataset.queries_iter())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-terrier # install pyTerrier\n",
        "\n",
        "import pyterrier as pt\n",
        "\n",
        "# Initalize pyTerrier if not started\n",
        "if not pt.started():\n",
        "    pt.init()\n",
        "\n",
        "# Ask pyTerrier to download the dataset, the 'irds:' header tells pyTerrier to use ir_datasets as the data source\n",
        "pyTerrierDataset = pt.get_dataset('irds:crisisfacts/009/2020-08-04')\n",
        "\n",
        "# To create the index, we use an 'indexer', this interates over the documents in the collection and adds them to the index\n",
        "# The paramters of this call are:\n",
        "#  Index Storage Path: \"None\" (some index types write to disk, this would be the directory to write to)\n",
        "#  Index Type: type=pt.index.IndexingType(3) (Type 3 is a Memory Index)\n",
        "#  Meta Index Fields: meta=['docno', 'text'] (The index also can store raw fields so they can be attached to the search results, this specifies what fields to store)\n",
        "#  Meta Index Lengths: meta_lengths=[40, 200] (pyTerrier allocates a fixed amount of storage space per field, how many characters should this be?)\n",
        "indexer = pt.IterDictIndexer(\"None\", type=pt.index.IndexingType(3), meta=['docno', 'text'], meta_lengths=[40, 200])\n",
        "\n",
        "# Trigger the indexing process\n",
        "index = indexer.index(pyTerrierDataset.get_corpus_iter())\n",
        "\n",
        "retriever = pt.BatchRetrieve(index, wmodel=\"DFReeKLIM\", metadata=[\"docno\", \"text\"])\n",
        "\n",
        "pd.DataFrame(retriever.search(\"injuries\"))\n",
        "\n",
        "# All of the above codes are provided by project at https://colab.research.google.com/github/crisisfacts/utilities/blob/main/00-Data/00-CrisisFACTS.Downloader.ipynb"
      ],
      "metadata": {
        "id": "VBrPT_AC3Okh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data for Doc2Vec\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(itemsAsDataFrame['text'].str.split())]\n",
        "\n",
        "# Train a Doc2Vec model\n",
        "model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "document_embeddings = np.array([model.infer_vector(doc.words) for doc in documents])\n",
        "\n",
        "# Function to calculate cosine similarity between two vectors\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Generate an embedding for a query\n",
        "query = \"injuries\"\n",
        "query_embedding = model.infer_vector(query.split())\n",
        "\n",
        "# Calculate similarity scores between the query and all documents\n",
        "similarity_scores = [cosine_similarity(query_embedding, doc_embedding) for doc_embedding in document_embeddings]\n",
        "\n",
        "# Print the top 10 documents with the highest similarity scores\n",
        "top_docs = np.argsort(similarity_scores)[::-1][:10]\n",
        "print(itemsAsDataFrame.iloc[top_docs])"
      ],
      "metadata": {
        "id": "Tt0c5VxM3Uj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-extractive-summarizer\n",
        "\n",
        "from summarizer import Summarizer\n",
        "\n",
        "# Instantiate a Summarizer model\n",
        "model = Summarizer()\n",
        "\n",
        "# Let's say we want to summarize the top 10 documents related to \"injuries\"\n",
        "top_docs_text = itemsAsDataFrame.iloc[top_docs]['text']\n",
        "\n",
        "# Concatenate the text of the top documents into one string\n",
        "text_to_summarize = ' '.join(top_docs_text)\n",
        "\n",
        "# Use the model to summarize the text\n",
        "summary = model(text_to_summarize)\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "4OhTGoJt6j32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def process_request(request):\n",
        "    # This is a placeholder function.\n",
        "    return [\n",
        "        {\"doc_id\": \"doc1\", \"score\": 0.9},\n",
        "        {\"doc_id\": \"doc2\", \"score\": 0.8},\n",
        "        # ... more documents ...\n",
        "    ]\n",
        "\n",
        "results = []\n",
        "for eventNo in eventNoList:\n",
        "    dailyInfo = getDaysForEventNo(eventNo)\n",
        "    for day in dailyInfo:\n",
        "        request_id = day[\"requestID\"]\n",
        "        ranked_docs = process_request(request_id)\n",
        "        for rank, doc in enumerate(ranked_docs, start=1):\n",
        "            result = {\n",
        "                \"run_id\": \"run1\",  # placeholder\n",
        "                \"event_id\": day[\"eventID\"],\n",
        "                \"request_id\": request_id,\n",
        "                \"doc_id\": doc[\"doc_id\"],\n",
        "                \"rank\": rank,\n",
        "                \"score\": doc[\"score\"],\n",
        "                \"run_type\": \"automatic\"  # placeholder\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "# Write the results to the output file\n",
        "with open('submission.json', 'w') as f:\n",
        "    for result in results:\n",
        "        json.dump(result, f)\n",
        "        f.write('\\n')  # write a newline character after each JSON object\n",
        "\n",
        "with open('submission.json', 'r') as f:\n",
        "    for line in f:\n",
        "        print(line)\n"
      ],
      "metadata": {
        "id": "2hu07QbY_UIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Score is precision @ k\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    \"\"\"\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k] != 0\n",
        "    return np.mean(r)\n",
        "\n",
        "def dcg_at_k(r, k):\n",
        "    \"\"\"Score is discounted cumulative gain (dcg)\n",
        "    Relevance is positive real values.\n",
        "    \"\"\"\n",
        "    r = np.asarray(r)[:k]\n",
        "    if r.size:\n",
        "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "    return 0.\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\"\"\"\n",
        "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
        "    if not dcg_max:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k) / dcg_max\n",
        "\n",
        "def expected_reciprocal_rank(r):\n",
        "    \"\"\"ERR is the expected reciprocal rank\"\"\"\n",
        "    p = 1.0\n",
        "    for i in range(len(r)):\n",
        "        rank = i+1\n",
        "        R = (2**r[i]-1) / 2**max(r)\n",
        "        p *= R\n",
        "        ERR = p / rank\n",
        "    return ERR\n",
        "\n",
        "# Assumption for relevance scores for a query\n",
        "relevance_scores = [3, 2, 3, 0, 0, 1, 2, 3, 2, 0]\n",
        "\n",
        "# Calculate the metrics:\n",
        "print(\"P@10:\", precision_at_k(relevance_scores, 10))\n",
        "print(\"nDCG@10:\", ndcg_at_k(relevance_scores, 10))\n",
        "print(\"ERR:\", expected_reciprocal_rank(relevance_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBLXXk5NAXqy",
        "outputId": "db460971-5eae-43a4-feec-cbed61cf2b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P@10: 0.7\n",
            "nDCG@10: 0.9183997457184155\n",
            "ERR: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline is our first prototype for the TREC 2023 CrisisFACTS challenge, which aims to develop systems that can effectively retrieve and analyze information about crisis events. The specific requirements of the challenge can be found on the [CrisisFACTS website](https://crisisfacts.github.io/).\n",
        "\n",
        "Here's a breakdown of what each section does:\n",
        "\n",
        "1. **Installation and Authentication**: The code begins by installing necessary packages and setting up authentication for accessing the CrisisFACTS dataset. It uses the `ir_datasets` package, which is a collection of information retrieval datasets. The credentials are stored in a JSON file.\n",
        "Provided by the Project website at: https://colab.research.google.com/github/crisisfacts/utilities/blob/main/00-Data/00-CrisisFACTS.Downloader.ipynb\n",
        "\n",
        "2. **Event List and Metadata Retrieval**: The code then defines a list of event numbers, each representing a different crisis event. For each event, it retrieves metadata about the event, such as the dates on which the event occurred.\n",
        "Provided by the Project website at: https://colab.research.google.com/github/crisisfacts/utilities/blob/main/00-Data/00-CrisisFACTS.Downloader.ipynb\n",
        "\n",
        "3. **Data Loading and Exploration**: The code loads the data for each event and prints out the first few items. It then converts the data into a pandas DataFrame and filters the data based on the source type (e.g., Reddit, Twitter, Facebook, News).\n",
        "Provided by the Project website at: https://colab.research.google.com/github/crisisfacts/utilities/blob/main/00-Data/00-CrisisFACTS.Downloader.ipynb\n",
        "\n",
        "4. **Indexing and Retrieval**: The code uses the `pyTerrier` package to index the data and retrieve relevant documents. It uses the `BatchRetrieve` class with the `DFReeKLIM` weighting model to retrieve documents.\n",
        "Provided by the Project website at: https://colab.research.google.com/github/crisisfacts/utilities/blob/main/00-Data/00-CrisisFACTS.Downloader.ipynb\n",
        "\n",
        "5. **Document Embedding and Similarity Calculation**: The code uses the `gensim` package to train a Doc2Vec model on the text of the documents. It then calculates the cosine similarity between a query and all documents, and prints out the top 10 most similar documents.\n",
        "\n",
        "6. **Summarization**: The code uses the `bert-extractive-summarizer` package to summarize the text of the top 10 most similar documents.\n",
        "\n",
        "7. **Result Generation and Submission**: The code generates a JSON file containing the ranked list of documents for each request. This file is what is expected to be submitted to the TREC 2023 CrisisFACTS challenge.\n",
        "\n",
        "8. **Evaluation Metrics**: The code defines several functions for calculating information retrieval evaluation metrics, such as precision at k, normalized discounted cumulative gain (nDCG) at k, and expected reciprocal rank (ERR).\n"
      ],
      "metadata": {
        "id": "7r-Myp1RSFij"
      }
    }
  ]
}